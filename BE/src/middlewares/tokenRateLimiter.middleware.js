/**
 * Token Rate Limiter Middleware
 * Giá»›i háº¡n token usage cho OpenAI Assistant API
 * Limit: 80,000 TPM (tokens per minute) - Ä‘á»ƒ buffer 20k cho quota 100k
 */

// Token tracking store
const tokenStore = {
    tokens: [],           // Array of { timestamp, count }
    windowMs: 60 * 1000,  // 1 minute window
    maxTokens: 80000,     // 80k TPM limit (buffer 20k from 100k quota)
};

/**
 * Æ¯á»›c tÃ­nh sá»‘ token tá»« message
 * Rule of thumb: ~4 characters = 1 token (cho tiáº¿ng Viá»‡t cÃ³ thá»ƒ cao hÆ¡n)
 */
export const estimateTokens = (text) => {
    if (!text) return 0;
    // Tiáº¿ng Viá»‡t thÆ°á»ng cáº§n nhiá»u token hÆ¡n tiáº¿ng Anh
    // Æ¯á»›c tÃ­nh: 1 token â‰ˆ 3 kÃ½ tá»± cho tiáº¿ng Viá»‡t
    return Math.ceil(text.length / 3);
};

/**
 * Æ¯á»›c tÃ­nh token cho má»™t request chatbot
 * Bao gá»“m: input message + context + estimated response
 */
export const estimateRequestTokens = (message, history = []) => {
    let totalTokens = 0;

    // Input message
    totalTokens += estimateTokens(message);

    // History context (náº¿u cÃ³)
    if (history && history.length > 0) {
        history.forEach(msg => {
            totalTokens += estimateTokens(msg.content || msg.text || '');
        });
    }

    // System prompt overhead (khoáº£ng 500 tokens)
    totalTokens += 500;

    // Estimated response (trung bÃ¬nh 300-500 tokens)
    totalTokens += 400;

    // Function calling overhead
    totalTokens += 200;

    return totalTokens;
};

/**
 * Clean up expired tokens from the window
 */
const cleanupExpiredTokens = () => {
    const now = Date.now();
    const windowStart = now - tokenStore.windowMs;

    // Remove tokens outside the window
    tokenStore.tokens = tokenStore.tokens.filter(t => t.timestamp > windowStart);
};

/**
 * Get current token usage in the window
 */
export const getCurrentUsage = () => {
    cleanupExpiredTokens();
    return tokenStore.tokens.reduce((sum, t) => sum + t.count, 0);
};

/**
 * Get remaining tokens available
 */
export const getRemainingTokens = () => {
    return Math.max(0, tokenStore.maxTokens - getCurrentUsage());
};

/**
 * Check if we can process a request with estimated tokens
 */
export const canProcessRequest = (estimatedTokens) => {
    const remaining = getRemainingTokens();
    return remaining >= estimatedTokens;
};

/**
 * Record token usage
 */
export const recordTokenUsage = (tokenCount) => {
    cleanupExpiredTokens();
    tokenStore.tokens.push({
        timestamp: Date.now(),
        count: tokenCount,
    });

    console.log(`ðŸ“Š Token recorded: ${tokenCount}, Total in window: ${getCurrentUsage()}/${tokenStore.maxTokens}`);
};

/**
 * Get time until tokens are available
 */
export const getWaitTime = (requiredTokens) => {
    cleanupExpiredTokens();

    if (canProcessRequest(requiredTokens)) {
        return 0;
    }

    // Find oldest token that needs to expire
    const sortedTokens = [...tokenStore.tokens].sort((a, b) => a.timestamp - b.timestamp);
    let tokensToFree = requiredTokens - getRemainingTokens();
    let waitUntil = Date.now();

    for (const token of sortedTokens) {
        tokensToFree -= token.count;
        waitUntil = token.timestamp + tokenStore.windowMs;

        if (tokensToFree <= 0) break;
    }

    return Math.max(0, waitUntil - Date.now());
};

/**
 * Rate Limiter Middleware
 */
export const tokenRateLimiter = (req, res, next) => {
    const message = req.body?.message || '';
    const history = req.body?.history || [];

    // Estimate tokens for this request
    const estimatedTokens = estimateRequestTokens(message, history);

    // Check if we can process
    if (!canProcessRequest(estimatedTokens)) {
        const waitTime = getWaitTime(estimatedTokens);
        const waitSeconds = Math.ceil(waitTime / 1000);

        console.log(`âš ï¸ Rate limit reached. Current: ${getCurrentUsage()}/${tokenStore.maxTokens} TPM`);
        console.log(`â³ Need to wait ${waitSeconds}s for ${estimatedTokens} tokens`);

        return res.status(429).json({
            success: false,
            error: 'Rate limit exceeded',
            message: `Há»‡ thá»‘ng Ä‘ang báº­n, vui lÃ²ng thá»­ láº¡i sau ${waitSeconds} giÃ¢y`,
            details: {
                currentUsage: getCurrentUsage(),
                maxTokens: tokenStore.maxTokens,
                estimatedTokens,
                waitTimeMs: waitTime,
                waitTimeSeconds: waitSeconds,
            }
        });
    }

    // Store estimated tokens in request for later recording
    req.estimatedTokens = estimatedTokens;

    console.log(`âœ… Request allowed. Estimated: ${estimatedTokens} tokens, Available: ${getRemainingTokens()}/${tokenStore.maxTokens}`);

    next();
};

/**
 * Record actual token usage after response
 * Call this in controller after getting response from OpenAI
 */
export const recordActualUsage = (usage) => {
    if (usage && (usage.total_tokens || usage.prompt_tokens || usage.completion_tokens)) {
        const totalTokens = usage.total_tokens ||
            ((usage.prompt_tokens || 0) + (usage.completion_tokens || 0));
        recordTokenUsage(totalTokens);
        return totalTokens;
    }
    return 0;
};

/**
 * Get rate limit status
 */
export const getRateLimitStatus = () => {
    cleanupExpiredTokens();
    const currentUsage = getCurrentUsage();
    const remaining = getRemainingTokens();
    const percentage = Math.round((currentUsage / tokenStore.maxTokens) * 100);

    return {
        currentUsage,
        maxTokens: tokenStore.maxTokens,
        remaining,
        usagePercentage: percentage,
        windowMs: tokenStore.windowMs,
        requestsInWindow: tokenStore.tokens.length,
    };
};

export default tokenRateLimiter;
